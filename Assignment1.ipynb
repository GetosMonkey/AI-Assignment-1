{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d7271d-d22a-4d59-9414-7fca927fb6b5",
   "metadata": {},
   "source": [
    "# CSC2042S 2025 Assignment 1: Unsupervised Learning\n",
    "### Maryam Abrahams\n",
    "### ABRMAR043\n",
    "### 18th August 2025\n",
    "\n",
    "## Part 1:  Data Preprocessing\n",
    "\n",
    "Conducting exploratory analysis and visualising data using t-SNE dimensionality reduction and the scikit-learn toolbox. Part and parcel of this analysis is the proper documentation alongside it, including justifications, characteristics, and the impact of each preprocessing decision.\n",
    "\n",
    "T-SNE (t-distributed stochastic neighbour embedding ) takes a high-dimensional dataset and reduces it to a low-dimensional graph, which retains a lot of the data. It does so largely through clustering and maintaining variance, similar to the process of Principal Component Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3c4d3abe-e82f-4af1-ae52-e67c2602e2d6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country Name</th>\n",
       "      <th>Country Code</th>\n",
       "      <th>Indicator Name</th>\n",
       "      <th>Indicator Code</th>\n",
       "      <th>1960</th>\n",
       "      <th>1961</th>\n",
       "      <th>1962</th>\n",
       "      <th>1963</th>\n",
       "      <th>1964</th>\n",
       "      <th>1965</th>\n",
       "      <th>...</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "      <th>2022</th>\n",
       "      <th>2023</th>\n",
       "      <th>2024</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Access to clean fuels and technologies for coo...</td>\n",
       "      <td>EG.CFT.ACCS.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>18.001597</td>\n",
       "      <td>18.558234</td>\n",
       "      <td>19.043572</td>\n",
       "      <td>19.586457</td>\n",
       "      <td>20.192064</td>\n",
       "      <td>20.828814</td>\n",
       "      <td>21.372164</td>\n",
       "      <td>22.100884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Access to clean fuels and technologies for coo...</td>\n",
       "      <td>EG.CFT.ACCS.RU.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.096003</td>\n",
       "      <td>7.406706</td>\n",
       "      <td>7.666648</td>\n",
       "      <td>8.020952</td>\n",
       "      <td>8.403358</td>\n",
       "      <td>8.718306</td>\n",
       "      <td>9.097176</td>\n",
       "      <td>9.473374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Access to clean fuels and technologies for coo...</td>\n",
       "      <td>EG.CFT.ACCS.UR.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>38.488233</td>\n",
       "      <td>38.779953</td>\n",
       "      <td>39.068462</td>\n",
       "      <td>39.445526</td>\n",
       "      <td>39.818645</td>\n",
       "      <td>40.276374</td>\n",
       "      <td>40.687817</td>\n",
       "      <td>41.211606</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Access to electricity (% of population)</td>\n",
       "      <td>EG.ELC.ACCS.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>33.922276</td>\n",
       "      <td>38.859598</td>\n",
       "      <td>40.223744</td>\n",
       "      <td>43.035073</td>\n",
       "      <td>44.390861</td>\n",
       "      <td>46.282371</td>\n",
       "      <td>48.127211</td>\n",
       "      <td>48.801258</td>\n",
       "      <td>50.668330</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Africa Eastern and Southern</td>\n",
       "      <td>AFE</td>\n",
       "      <td>Access to electricity, rural (% of rural popul...</td>\n",
       "      <td>EG.ELC.ACCS.RU.ZS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>16.527554</td>\n",
       "      <td>24.627753</td>\n",
       "      <td>25.432092</td>\n",
       "      <td>27.061929</td>\n",
       "      <td>29.154282</td>\n",
       "      <td>31.022083</td>\n",
       "      <td>32.809138</td>\n",
       "      <td>33.783960</td>\n",
       "      <td>35.375216</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Country Name Country Code  \\\n",
       "0  Africa Eastern and Southern          AFE   \n",
       "1  Africa Eastern and Southern          AFE   \n",
       "2  Africa Eastern and Southern          AFE   \n",
       "3  Africa Eastern and Southern          AFE   \n",
       "4  Africa Eastern and Southern          AFE   \n",
       "\n",
       "                                      Indicator Name     Indicator Code  1960  \\\n",
       "0  Access to clean fuels and technologies for coo...     EG.CFT.ACCS.ZS   NaN   \n",
       "1  Access to clean fuels and technologies for coo...  EG.CFT.ACCS.RU.ZS   NaN   \n",
       "2  Access to clean fuels and technologies for coo...  EG.CFT.ACCS.UR.ZS   NaN   \n",
       "3            Access to electricity (% of population)     EG.ELC.ACCS.ZS   NaN   \n",
       "4  Access to electricity, rural (% of rural popul...  EG.ELC.ACCS.RU.ZS   NaN   \n",
       "\n",
       "   1961  1962  1963  1964  1965  ...       2015       2016       2017  \\\n",
       "0   NaN   NaN   NaN   NaN   NaN  ...  18.001597  18.558234  19.043572   \n",
       "1   NaN   NaN   NaN   NaN   NaN  ...   7.096003   7.406706   7.666648   \n",
       "2   NaN   NaN   NaN   NaN   NaN  ...  38.488233  38.779953  39.068462   \n",
       "3   NaN   NaN   NaN   NaN   NaN  ...  33.922276  38.859598  40.223744   \n",
       "4   NaN   NaN   NaN   NaN   NaN  ...  16.527554  24.627753  25.432092   \n",
       "\n",
       "        2018       2019       2020       2021       2022       2023  2024  \n",
       "0  19.586457  20.192064  20.828814  21.372164  22.100884        NaN   NaN  \n",
       "1   8.020952   8.403358   8.718306   9.097176   9.473374        NaN   NaN  \n",
       "2  39.445526  39.818645  40.276374  40.687817  41.211606        NaN   NaN  \n",
       "3  43.035073  44.390861  46.282371  48.127211  48.801258  50.668330   NaN  \n",
       "4  27.061929  29.154282  31.022083  32.809138  33.783960  35.375216   NaN  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports, setup, and understanding our data\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "from pylab import rcParams\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv(\"WDICSV.csv\")\n",
    "display(df.head())\n",
    "\n",
    "# To preprocess the data, we want to take this high-dimensional dataset and reduce it to a low-dimensional one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94252f6-c4c5-4d1b-a9ef-9acd6c18c343",
   "metadata": {},
   "source": [
    "To perform t-SNE on our dataset, we must first reshape the data in such a way that t-SNE will be effective, removing missing features and very incomplete country years so that we don't have such a massive amount of missing data as well as reshaping the data to be better for clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691cdcee-d8aa-4950-88cd-8a03249d0e53",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Reshaping so that there are (Country-year) rows:\n",
    "df_long = df.melt(\n",
    "    id_vars=['Country Name', 'Country Code', 'Indicator Name', 'Indicator Code'],\n",
    "    var_name = 'Year', \n",
    "    value_name = 'Value'\n",
    ")\n",
    "\n",
    "df_wide = df_long.pivot_table(\n",
    "    index = ['Country Name', 'Country Code', 'Year'],\n",
    "    columns = 'Indicator Code', \n",
    "    values = 'Value'\n",
    ").reset_index()\n",
    "\n",
    "# Testing different thresholds: \n",
    "print(\"Row completeness per threshold:\")\n",
    "thresholds = [0.5, 0.6, 0.7, 0.8]\n",
    "for t in thresholds:\n",
    "    mask = df_wide.drop(['Country Name', 'Country Code', 'Year'], axis=1).notna().mean(axis=1) >= t\n",
    "    remaining = len(df_wide[mask])\n",
    "    print(f\"   Threshold {t*100:.0f}% -> {remaining} rows kept ({(remaining/len(df_wide))*100:.1f}%)\")\n",
    "    \n",
    "# Data Collection for Analysis & Justification:\n",
    "og_shape = df_wide.shape\n",
    "print(\"Original shape of pivoted dataset: \" + str(og_shape))\n",
    "print(\"Original # of country-year observations: \" + str(og_shape[0]))\n",
    "print(\"Original # of features (x): \" + str(og_shape[1] - 3) + \"\\n\")\n",
    "\n",
    "# Cleaning the missing data:\n",
    "numeric_pt = df_wide.drop(['Country Name', 'Country Code', 'Year'], axis=1).notna().mean(axis=1) >= 0.7\n",
    "df_wide = df_wide[numeric_pt]\n",
    "\n",
    "features = df_wide.drop(['Country Name', 'Country Code', 'Year'], axis=1)\n",
    "features = features.fillna(features.mean())\n",
    "\n",
    "# first I must drop entirely empty rows \n",
    "features = features.dropna(axis=1, how='all')\n",
    "print(\"Data shape after dropping completely empty columns: \" + str(features.shape))\n",
    "\n",
    "# Conducting t-SNE dimensionality reduction on our dataframe:\n",
    "# resources used: https://youtu.be/85XaciPBCkw\n",
    "\n",
    "# Scaling features:\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "m = TSNE(n_components=2, learning_rate = 50)\n",
    "tsne_features = m.fit_transform(features_scaled)\n",
    "\n",
    "df_wide['x'] = tsne_features[:,0]\n",
    "df_wide['y'] = tsne_features[:,1]\n",
    "\n",
    "# Ordering countries by value instead of numerically:\n",
    "#country_order = df_wide.groupby('Indicator Code')['x'].mean().sort_values().index\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.scatterplot(x='x', y ='y', hue='Country Code', data = df_wide)\n",
    "plt.title(\"t-SNE Analysis of Country vs Year Development Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e82c79-232d-4e79-9482-2c4cc16ede02",
   "metadata": {},
   "source": [
    "## Part 2: K-means clustering; Initialization\n",
    "\n",
    "Selecting k initial centroids randomly from the dataset, where k is the number of clusters you want to find. We want to only use numpy data structures for our K-means clustering implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd1f89-c2ac-4d71-807d-1fe0431fbb38",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Using the K-means clustering tutorial memo as a guideline: \n",
    "# Setup: \n",
    "\n",
    "def initialise_centroids(X, k):\n",
    "    # Choose k unique random indices\n",
    "    indices = random.sample(range(len(X)), k)\n",
    "    return [X[i] for i in indices]\n",
    "    \n",
    "def compute_cluster_mean(cluster_list):\n",
    "    #For null values\n",
    "    if len(cluster_list) == 0:\n",
    "        return None \n",
    "    return np.mean(cluster_list, axis=0)\n",
    "\n",
    "def compute_euclidean_distance(x1, x2):\n",
    "    return np.linalg.norm(x1 - x2)\n",
    "\n",
    "def form_new_clusters(X, centroids):\n",
    "    clusters = [[] for _ in centroids]\n",
    "    for x in X:\n",
    "        centroid_distances = [compute_euclidean_distance(x, c) for c in centroids]\n",
    "        new_cluster = np.argmin(centroid_distances)\n",
    "        clusters[new_cluster].append(x)\n",
    "    return clusters\n",
    "\n",
    "def compute_new_centroids(cluster_list):\n",
    "    centroids = []\n",
    "    for cluster_data in cluster_list:\n",
    "        new_centroid = compute_cluster_mean(cluster_data)\n",
    "        centroids.append(new_centroid)\n",
    "    return centroids\n",
    "\n",
    "def repeat_until_convergence(X, cluster_list, centroids):\n",
    "    centroid_diff = 10000\n",
    "    count = 0 #keeping track of the number of iterations\n",
    "    while centroid_diff > 0:\n",
    "        new_cluster_list = form_new_clusters(X, centroids)\n",
    "        new_centroids = compute_new_centroids(new_cluster_list)\n",
    "        centroid_diff = max([compute_euclidean_distance(centroids[i], new_centroids[i]) for i in range(len(centroids))])\n",
    "        centroids = new_centroids\n",
    "        cluster_list = new_cluster_list\n",
    "        print(centroid_diff)\n",
    "        count += 1\n",
    "    return cluster_list, centroids, count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74eb3c5-49f2-4ed2-9c5b-20598b65736b",
   "metadata": {},
   "source": [
    "After this setup we now want to implement and compare multiple initialization strategies to understand the effects that differing initializaton will have on the end clustering result. We want to implement both random initialization, using multiple different random seeds and running his test multiple times, as well as K-means ++ initialization, selecting initial centres that are very far apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d270b51-d1d0-4c74-ad1b-03626ae25874",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Random Initialization:\n",
    "\n",
    "X = features_scaled\n",
    "k = 10 \n",
    "\n",
    "# For different seeds:\n",
    "random_results = []\n",
    "\n",
    "for seed in [1, 2, 3, 4, 5]:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    print(f\"\\n--- Seed {seed} ---\")\n",
    "    centroids = initialise_centroids(X, k)\n",
    "    cluster_list = form_new_clusters(X, centroids)\n",
    "    final_clusters, final_centroids, iterations = repeat_until_convergence(X, cluster_list, centroids)\n",
    "\n",
    "    #loss function\n",
    "    loss = 0\n",
    "    for j in range(len(final_clusters)):\n",
    "        for point in final_clusters[j]:\n",
    "            loss += compute_euclidean_distance(point, final_centroids[j]**2)\n",
    "    loss = round(loss, 2)\n",
    "\n",
    "    print()\n",
    "    if iterations > 1 and iterations != 0: \n",
    "        print(\"It took \" + str(iterations) + \" iterations for the algorithm to converge with random initialization.\")\n",
    "        print(f\"With final loss: {loss}\")\n",
    "    else: \n",
    "        print(\"It took \" + str(iterations) + \" iteration for the algorithm to converge with random initialization.\")\n",
    "        print(f\"With final loss: {loss}\")\n",
    "\n",
    "    random_results.append({'seed': seed, 'iterations': iterations, 'loss': loss})\n",
    "\n",
    "    print(\"\\n--- Plot ---\")\n",
    "    colors = sns.color_palette(\"pastel\", k)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    for j, cluster in enumerate(final_clusters):\n",
    "        cluster = np.array(cluster)\n",
    "        plt.scatter(cluster[:, 0], cluster[:,1], s=30, color=colors[j], label=f\"Cluster {j}\")\n",
    "    final_centroids = np.array(final_centroids)\n",
    "    plt.scatter(final_centroids[:, 0], final_centroids[:, 1], c=\"black\", s=30, marker=\"x\", label=\"Centroids\")\n",
    "    plt.title(f\"Clusters for Seed {seed}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c985cb-cfb9-4527-b46f-a989d4c2fa43",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialization via K-means++\n",
    "\n",
    "def k_means_plus_plus(X,k):\n",
    "    n_samples = X.shape[0]\n",
    "    centroids = []\n",
    "    c_one = np.random.randint(0, n_samples) \n",
    "    centroids.append(X[c_one])\n",
    "\n",
    "    for i in range(1,k):  \n",
    "        distances = np.array([min(np.linalg.norm(x-c)**2 for c in centroids) for x in X])\n",
    "        probs = distances/distances.sum()\n",
    "        c_new = np.random.choice(n_samples, p = probs) \n",
    "        centroids.append(X[c_new])\n",
    "        \n",
    "    return centroids\n",
    "\n",
    "# Running our K-means++\n",
    "kpp_results = []\n",
    "\n",
    "for seed in [1, 2, 3, 4, 5]:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    print(f\"\\n--- Seed {seed} ---\")\n",
    "    centroids = k_means_plus_plus(X, k)\n",
    "    cluster_list = form_new_clusters(X, centroids)\n",
    "    final_clusters, final_centroids, iterations = repeat_until_convergence(X, cluster_list, centroids)\n",
    "    \n",
    "    #loss function\n",
    "    loss = 0\n",
    "    for j in range(len(final_clusters)):\n",
    "        for point in final_clusters[j]:\n",
    "            loss += compute_euclidean_distance(point, final_centroids[j]**2)\n",
    "    loss = round(loss, 2)\n",
    "                 \n",
    "    print()\n",
    "    if iterations > 1: \n",
    "        print(\"It took \" + str(iterations) + \" iterations for the algorithm to converge with k-means++.\")\n",
    "        print(f\"With final loss: {loss}\")\n",
    "    else: \n",
    "        print(\"It took \" + str(iterations) + \" iteration for the algorithm to converge with random k_means++.\")\n",
    "        print(f\"With final loss: {loss}\")\n",
    "\n",
    "    kpp_results.append({'seed': seed, 'iterations': iterations, 'loss': loss})\n",
    "\n",
    "    print(\"\\n--- Plot ---\")\n",
    "    colors = sns.color_palette(\"pastel\", k)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    for j, cluster in enumerate(final_clusters):\n",
    "        cluster = np.array(cluster)\n",
    "        plt.scatter(cluster[:, 0], cluster[:,1], s=30, color=colors[j], label=f\"Cluster {j}\")\n",
    "    final_centroids = np.array(final_centroids)\n",
    "    plt.scatter(final_centroids[:, 0], final_centroids[:, 1], c=\"black\", s=30, marker=\"x\", label=\"Centroids\")\n",
    "    plt.title(f\"Clusters for Seed {seed}\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96db5295-8ab1-424c-b589-49cc6a489e99",
   "metadata": {},
   "source": [
    "### Cluster Analysis: \n",
    "Visualizing the cluster centroids for each cluster in both the random and k-means++ initialization cases. From the similar spread of the centroids across different tests, we can see that there is a stability of cluster assignments across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d4ee65-44ed-40fd-aa81-05613c05fa3f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Visualizing the final centroids & Summary\n",
    "print(\"SUMMARY COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "avg_random_iter = np.mean([r['iterations'] for r in random_results])\n",
    "avg_random_loss = np.mean([r['loss'] for r in random_results])\n",
    "std_random_loss = np.std([r['loss'] for r in random_results])\n",
    "\n",
    "avg_kpp_iter = np.mean([r['iterations'] for r in kpp_results])\n",
    "avg_kpp_loss = np.mean([r['loss'] for r in kpp_results])\n",
    "std_kpp_loss = np.std([r['loss'] for r in kpp_results])\n",
    "\n",
    "print(f\"Random Initialization:\")\n",
    "print(f\"  Avg. Iterations: {avg_random_iter:.1f}, Avg. Loss: {avg_random_loss:.2f} (±{std_random_loss:.2f})\")\n",
    "print(f\"K-means++ Initialization:\")\n",
    "print(f\"  Avg. Iterations: {avg_kpp_iter:.1f}, Avg. Loss: {avg_kpp_loss:.2f} (±{std_kpp_loss:.2f})\")\n",
    "\n",
    "def plot_initial(X, centroids, title):\n",
    "    centroids = np.array(centroids)\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.scatter(X[:, 0], X[:,1], s=30, c=\"lightblue\", label=\"Data\")\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], c=\"blue\", s=30, marker=\"x\", label=\"Centroids\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "seeds = [69, 420, 777, 1234]\n",
    "for i, seed in enumerate(seeds, 1):\n",
    "    print(\"=\"*120)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    c_random = initialise_centroids(X, k)\n",
    "    c_kmeans = k_means_plus_plus(X, k) \n",
    "    plot_initial(X, c_random, f\"Random Initialization {i}\")\n",
    "    plot_initial(X, c_kmeans, f\"Initialization with K-means++ {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2927e7-25d9-42fe-9a77-c53a93bd44d9",
   "metadata": {},
   "source": [
    "## Part 3: Convergence Criteria\n",
    "\n",
    "Implementing and evaluating multiple different convergence criteria, including: the maximum iterations, the centroid change threshold, and the inertia change threshold.\n",
    "\n",
    "Importantly, we want to analyze how each of the above criteria affects the trade-off between computational cost and the solution's quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a0d9b3-7b0c-49db-94d4-42f9b6c1f161",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# k-means irrespective of stopping criteria\n",
    "\n",
    "def kmeans(X, k, c_initial, max_it, c_change_min, inertia_change_min, stop = \"max_it\"): \n",
    "    centroids = c_initial.copy()\n",
    "    inertia_history = []\n",
    "    shift_history = []\n",
    "    count = 0\n",
    "    \n",
    "    for it in range(max_it): \n",
    "        cluster_list = form_new_clusters(X, centroids) \n",
    "        c_new = compute_new_centroids(cluster_list)\n",
    "\n",
    "        #Handling empty clusters: \n",
    "        for i, centroid in enumerate(c_new):\n",
    "            if centroid is None: \n",
    "                c_new[i] = centroids[i]\n",
    "        \n",
    "        shift = max([compute_euclidean_distance(centroids[i], c_new[i]) for i in range(len(centroids))])\n",
    "        shift_history.append(shift) \n",
    "    \n",
    "        inertia = 0\n",
    "        for i, cluster in enumerate(cluster_list):\n",
    "            if cluster:  # Check if cluster is not empty\n",
    "                for point in cluster:\n",
    "                    inertia += compute_euclidean_distance(point, c_new[i])\n",
    "        inertia_history.append(inertia) \n",
    "\n",
    "        # Stop conditions initial\n",
    "        stop_early = False\n",
    "        if stop == 'centroid' and shift < c_change_min:\n",
    "            break\n",
    "        elif stop == 'inertia' and it > 0 and abs(inertia_history[-2] - inertia_history[-1]) < inertia_change_min:\n",
    "            break\n",
    "        elif stop == 'max_it' and it == max_it - 1:\n",
    "            break\n",
    "\n",
    "        centroids = c_new\n",
    "        count +=1\n",
    "\n",
    "        if stop_early: \n",
    "            break\n",
    "            \n",
    "    return cluster_list, centroids, count, inertia_history, shift_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c2583-0bb2-4139-9272-93246c143821",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Maximum Iterations, Centroid Change, and Inertia Change Checks: \n",
    "np.random.seed(1)\n",
    "c_initial = initialise_centroids(X,k)\n",
    "results = {}\n",
    "\n",
    "for mode in [\"max_it\", \"centroid\", \"inertia\"]: \n",
    "    results[mode] = kmeans(X, k, c_initial, max_it = 100, c_change_min = 0.001, inertia_change_min =1.0, stop = mode)\n",
    "\n",
    "# Summary for later analysis: \n",
    "print(\"Convergence Criteria\")\n",
    "print(\"=\"*50)\n",
    "header = f\"{'Criteria':<15} {'Iterations':<12} {'Final Inertia':<15} {'Final Shift':<12}\"\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for mode, (_, _, iters, inertia_hist, shift_hist) in results.items():\n",
    "    print(f\"{mode:<15} {iters:<12} {inertia_hist[-1]:<15.2f} {shift_hist[-1]:<12.6f}\")\n",
    "\n",
    "# Plotting comparisons:\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "# Inertia\n",
    "for mode, (_, _, _, inertia_history, _) in results.items():\n",
    "    axs[0].plot(range(len(inertia_history)), inertia_history, \"o-\", label=mode)\n",
    "axs[0].set_title(\"Inertia Trajectory\")\n",
    "axs[0].set_xlabel(\"Iteration\")\n",
    "axs[0].set_ylabel(\"Inertia\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Centroid shift\n",
    "for mode, (_, _, _, _, shift_history) in results.items():\n",
    "    axs[1].plot(range(len(shift_history)), shift_history, 'o-', label=mode)\n",
    "axs[1].set_title(\"Centroid Shift Trajectory\")\n",
    "axs[1].set_xlabel(\"Iteration\")\n",
    "axs[1].set_ylabel(\"Max Centroid Shift\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "axs[1].set_yscale('log') \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cbc30a-75c7-4767-a1f8-8a88f7ecb1e3",
   "metadata": {},
   "source": [
    "## Part 4: Determining the Optimal K\n",
    "\n",
    "For K-means clustering, the optimal number of clusters is a fundamental challenge. Instinctively, we'd want to pick k such that the loss function is minimized; however, we don't want to fall victim to overfitting. To combat this, we tend to use a hybrid approach of both manual and automated inspection of the loss function graph. In this situation we will be using the elbow method and the Silhoueete score method.\n",
    "\n",
    "resource: https://youtu.be/Qh7VxLsaU9M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754a81c2-2768-403c-8c6b-415b6f4c561b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comparing cluster quality with Silhouette scores\n",
    "K_range = range (2, 16)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in K_range: \n",
    "    c_initial = initialise_centroids(X, k)\n",
    "    clusters, centroids, count, inertia_history, shift_history = kmeans(X, k, c_initial, max_it=300, \n",
    "                                                                        c_change_min=0.0001, inertia_change_min =0.1, stop = 'centroid')\n",
    "    inertias.append(inertia_history[-1])\n",
    "    \n",
    "    #giving each data point a label\n",
    "    labels = np.zeros(len(X), dtype=int) \n",
    "    for cluster_idx, cluster_points in enumerate(clusters): \n",
    "        for p in cluster_points: \n",
    "            point_idx = np.where((X==p).all(axis=1))[0]\n",
    "            labels[point_idx] = cluster_idx\n",
    "\n",
    "    silhouettes = silhouette_score(X, labels) if k > 1 else None\n",
    "    silhouette_scores.append(silhouettes)\n",
    "\n",
    "k_optimal_index = np.argmax(silhouette_scores)\n",
    "k_optimal = K_range[k_optimal_index]\n",
    "\n",
    "# Plotting the Silhouette Scores Graph\n",
    "rcParams['figure.figsize'] = 16, 5\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(K_range, silhouette_scores, \"o-\", color = \"lavender\", \n",
    "            linewidth=3, markersize =7, label = \"Silhouette Curve\")\n",
    "plt.xlabel(\"$k$\", fontsize=14) \n",
    "plt.ylabel(\"Silhouette Score\", fontsize=14)\n",
    "plt.grid(which=\"major\", color=\"azure\", linestyle=\"--\") \n",
    "plt.title(\"Silhouette curve for predicting the optimal number of clusters (k)\", fontsize=14) \n",
    "\n",
    "plt.axvline(x=k_optimal, linestyle= \"--\", color = \"purple\", linewidth=3, \n",
    "             label=\"optimal number of clusters\")\n",
    "plt.scatter(k, silhouette_scores[k-2], c=\"magenta\", s=400)\n",
    "plt.legend(shadow=True)\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaca6b0-aec6-41f1-adde-cc70a5b8bbcd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Automating the elbow method: \n",
    "\n",
    "inertia_perc_change = [0]\n",
    "for i in range(1, len(inertias)):\n",
    "    change = (inertias[i-1] - inertias[i]) / inertias[i-1] * 100  # % decrease\n",
    "    inertia_perc_change.append(change)\n",
    "    \n",
    "threshold = 1.0\n",
    "k_elbow = None\n",
    "\n",
    "for k_idx, change in enumerate(inertia_perc_change):\n",
    "    if change < threshold:\n",
    "        k_elbow = K_range[k_idx]\n",
    "        break\n",
    "if k_elbow is None:\n",
    "    k_elbow = K_range[-1]\n",
    "\n",
    "print(\"Inertia Percentage Change per k:\")\n",
    "for k, change in zip(K_range, inertia_perc_change):\n",
    "    print(f\"k = {k}: {change:.2f}% decrease\")\n",
    "\n",
    "print(f\"\\nThe optimal number of clusters based on the elbow method (>{threshold}% change threshold) is k = {k_elbow}\")\n",
    "print(f\"The optimal number of clusters based on the silhouette score is k = {k_optimal}\")\n",
    "\n",
    "\n",
    "# Plotting the elbow method graphs for manual inspection with previous \"seeds\"\n",
    "fig, (ax1) = plt.subplots(1, figsize=(16, 5))\n",
    "\n",
    "for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    inertias_for_this_seed = []\n",
    "    for k in K_range: \n",
    "        c_initial = initialise_centroids(X, k)\n",
    "        clusters, centroids, count, inertia_history, shift_history = kmeans(X, k, c_initial, max_it=300, \n",
    "                                                                            c_change_min=0.0001, inertia_change_min=0.1, stop='centroid')\n",
    "        inertias_for_this_seed.append(inertia_history[-1])\n",
    "    plt.plot(K_range, inertias_for_this_seed, 'o-', linewidth=2, markersize=5, label=f'Seed {seed}')\n",
    "\n",
    "# Regular plot setup \n",
    "ax1.plot(K_range, inertias, \"bo-\", linewidth=2, markersize=7)\n",
    "ax1.set_xlabel(\"Number of clusters (k)\", fontsize=14)\n",
    "ax1.set_ylabel(\"Inertia\", fontsize=14)\n",
    "ax1.set_title(\"Elbow Method For Optimal k\", fontsize=14)\n",
    "ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "ax1.axvline(x=k_optimal, linestyle=\"--\", color=\"red\", linewidth=2, label=f'Silhouette Optimal k ({k_optimal})')\n",
    "ax1.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788204d4-8db6-4f3d-b96d-51f6ad888bae",
   "metadata": {},
   "source": [
    "## Part 5: Dimensionality reduction with PCA\n",
    "\n",
    "We want to apply PCA dimensionality reduction to our preprocessed data so that we can visualize our cluster in both a 2D and 3D PCA space so that, alongside our K-means clustering and t-SNE tests, we can better interpret the spread of the data and identify which original features most strongly contribute to each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2010c01-80c2-4743-ad30-523e53f28313",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Applying PCA to our dataset\n",
    "\n",
    "components_list = [1, 2, 3, 4, 5, 10, 20, 50, 100]\n",
    "pca_results = {}\n",
    "\n",
    "for c in components_list: \n",
    "    pca = PCA(n_components=c)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    pca_results[c] = { \"X_pca\":X_pca,\n",
    "                       \"explained_var\": pca.explained_variance_ratio_,\n",
    "                       \"components\":pca.components_}\n",
    "    print(f\"PCA with {c} components explains {np.sum(pca.explained_variance_ratio_)*100:.1f}% of the variance in the dataset\")\n",
    "\n",
    "# Top 3 principal components\n",
    "\n",
    "x_names = [f\"Feature {i}\" for i in range(X.shape[1])]\n",
    "\n",
    "for n, info in pca_results.items():\n",
    "    print(\"\\nTop 3 components for a \" + str(n) + \"D PCA:\")\n",
    "    for i in range(min(3, n)):\n",
    "        comp = info[\"components\"][i]\n",
    "        top_features = np.argsort(np.abs(comp))[::-1][:5]\n",
    "        top_features_names = [x_names[j] for j in top_features]\n",
    "        top_loadings = comp[top_features]\n",
    "        print(f\"PC{i+1}: {top_features_names}, loadings: {top_loadings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5636f33b-5d36-45d5-aec1-69f160add3f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 2D PCA Visualisation\n",
    "\n",
    "X_pca_2d = pca_results[2][\"X_pca\"]\n",
    "k = 10\n",
    "c_initial = initialise_centroids(X_pca_2d, k)\n",
    "clusters_pca, centroids_pca, iterations, inertia_hist, shift_hist = kmeans(\n",
    "    X_pca_2d, k, c_initial, max_it=300, c_change_min=1e-4, inertia_change_min=1e-4, stop=\"centroid\")\n",
    "\n",
    "labels = np.zeros(len(X_pca_2d), dtype=int)\n",
    "for cluster_idx, cluster_points in enumerate(clusters_pca):\n",
    "    for point in cluster_points:\n",
    "        idx = np.where((X_pca_2d == point).all(axis=1))[0]\n",
    "        labels[idx] = cluster_idx\n",
    "\n",
    "centroids_pca = np.array(centroids_pca)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca_2d[:,0], X_pca_2d[:,1], c=labels, cmap=\"tab10\", s=50)\n",
    "plt.scatter(centroids_pca[:,0], centroids_pca[:,1], c=\"black\", s=200, marker=\"x\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Clusters in 2D PCA space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1182791b-db14-4606-8ccf-bc3d8858b730",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 3D PCA Visualisation\n",
    "\n",
    "X_pca_3d = pca_results[3][\"X_pca\"]\n",
    "c_initial = initialise_centroids(X_pca_3d, k)\n",
    "clusters_3d, centroids_3d, iterations, _, _ = kmeans(\n",
    "    X_pca_3d, k, c_initial, max_it=300, c_change_min=1e-4, inertia_change_min=1e-4, stop=\"centroid\"\n",
    ")\n",
    "\n",
    "labels_3d = np.zeros(len(X_pca_3d), dtype=int)\n",
    "for cluster_idx, cluster_points in enumerate(clusters_3d):\n",
    "    for point in cluster_points:\n",
    "        idx = np.where((X_pca_3d == point).all(axis=1))[0]\n",
    "        labels_3d[idx] = cluster_idx\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_pca_3d[:,0], X_pca_3d[:,1], X_pca_3d[:,2], c=labels_3d, cmap=\"tab10\", s=50)\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\")\n",
    "plt.title(\"Clusters in 3D PCA space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be5f7f9-8e1a-47a8-b238-5937e18ee91a",
   "metadata": {},
   "source": [
    "## 5.1) PCA interpretation: \n",
    "\n",
    "What I accidentally did for Cluster interpretation, looking to see what parts of the PCA analysis, what factors go into defining the principal components and what part of the features explain most of the variance of our WDCI data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d4e60-8b3e-4cf9-b231-f6644aeed712",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Parsing the HDI \n",
    "hdi_df = pd.read_excel(\n",
    "    \"HDR25_Statistical_Annex_HDI_Table.xlsx\",\n",
    "    skiprows=4,\n",
    "    header=[0, 1]\n",
    ")\n",
    "\n",
    "hdi_df.columns = ['_'.join(filter(lambda x: str(x) != 'nan', map(str, col))).strip() \n",
    "                 for col in hdi_df.columns.values]\n",
    "\n",
    "#print(\"Flattened columns:\")\n",
    "#print(hdi_df.columns.tolist())\n",
    "#print(\"\\nFirst few rows:\")\n",
    "#print(hdi_df.head())\n",
    "\n",
    "# feature descriptions:\n",
    "feature_descriptions = {\n",
    "    # Demographic features (PC1)\n",
    "    'SP.POP.DPND.YG': 'Young dependency ratio',\n",
    "    'SP.DYN.CBRT.IN': 'Crude birth Rate',\n",
    "    'SP.POP.0004.MA.5Y': 'Male Populations age 0-4',\n",
    "    'SP.POP.DPND: 0.061': 'Total dependency ratio',\n",
    "    'SP.POP.0014.MA.ZS': 'Male population 0-14',\n",
    "    'SP.POP.1564.MA.ZS': 'Male population 15-64',\n",
    "    'SP.POP.1564.TO.ZS': 'Total population 15-64',\n",
    "    'SP.POP.4044.FE.5Y': 'Female population 40-44',\n",
    "    'SP.POP.1564.FE.ZS': 'Female population 15-64',\n",
    "    'SP.POP.4044.MA.5Y': 'Male population 40-44',\n",
    "    \n",
    "    # Economic features (PC2)\n",
    "    'SP.URB.TOTL':'Urban population total',\n",
    "    'NY.GNP.MKTP.PP.CD': 'Gross National Product',\n",
    "    'NY.GDP.MKTP.PP.CD': 'Gross Domestic Product',\n",
    "    'NV.AGR.TOTL.CD': 'Agricultural value added',\n",
    "    'NE.CON.PRVT.PP.CD': 'Household consumption',\n",
    "    'BN.GSR.FCTY.CD': 'Net income from abroad',\n",
    "    'EN.GHG.CO2.LU.FL.MT.CE.AR5': 'CO2 emissions from land use change',\n",
    "    'BN.FIN.TOTL.CD': 'Net financial transactions',\n",
    "    'EN.GHG.CO2.LU.OL.MT.CE.AR5': 'Other land use CO2 emissions',\n",
    "    'BN.CAB.XOKA.CD': 'Current account balance',\n",
    "\n",
    "    # Developmental and Capital features (PC3) \n",
    "    'NY.GNP.PCAP.CN': 'GNI per capita',\n",
    "    'NY.GDP.PCAP.CN': 'GDP per capita',\n",
    "    'NY.GDP.PCAP.KN': 'GDP per capita',\n",
    "    'NE.CON.GOVT.KN': 'General government final consumption expenditure',\n",
    "    'NV.SRV.TOTL.KN': 'Services, value added',\n",
    "    'MS.MIL.XPND.CN': 'Military expenditure',\n",
    "    'NE.CON.TOTL.KN': 'Final consumption expenditure',\n",
    "    'NY.GSR.NFCY.CN': 'Net income from abroad',\n",
    "    'DC.DAC.JPNL.CD': 'Net ODA received from Japan',\n",
    "    'SH.STA.SUIC.FE.P5': 'Suicide mortality rate, female',\n",
    "    'TM.VAL.MRCH.R4.ZS': 'Merchandise trade - Low & middle income',\n",
    "    'AG.LND.ARBL.ZS': 'Arable land',\n",
    "    'SE.SEC.DURS': 'Duration of secondary education',\n",
    "    'IP.PAT.RESD': 'Patent applications by residents',\n",
    "\n",
    "    # Inequality and Distribution features (PC4)\n",
    "    'SI.POV.GINI': 'Gini index',\n",
    "    'SI.DST.05TH.20': 'Income share held by the  lowest fifth',\n",
    "    'SI.DST.10TH.10': 'Income share held by highest 10%',\n",
    "    'SI.DST.50MD': 'Median income/consumption',\n",
    "    'PA.NUS.PPPC.RF': 'Price level ratio',\n",
    "    'TM.VAL.MRCH.R3.ZS': 'Merchandise trade',\n",
    "    'TX.VAL.MRCH.R3.ZS': 'Merchandise exports',\n",
    "    'SI.DST.02ND.20': 'Income share held by second 20%',\n",
    "    'SI.DST.FRST.20': 'Income share held by lowest 20%',\n",
    "    'SI.DST.FRST.10': 'Income share held by lowest 10%',\n",
    "    'SH.DYN.NCOM.MA.ZS': 'Male Mortality from disease',\n",
    "    'SI.DST.03RD.20': 'Income share held by third 20%',\n",
    "    'SH.DYN.NCOM.ZS': 'Total Mortality from disease',\n",
    "    'DC.DAC.NZLL.CD': 'Net ODA from New Zealand',\n",
    "    \n",
    "    # Labour and Gender features (PC5)\n",
    "    'SL.TLF.CACT.FM.ZS': 'Labor force participation rate, total',\n",
    "    'SL.TLF.ACTI.FE.ZS': 'Labor force, female',\n",
    "    'SL.TLF.TOTL.FE.ZS': 'Labor force, total',\n",
    "    'SL.TLF.CACT.FE.ZS': 'Labor force participation rate, female',\n",
    "    'SL.TLF.CACT.FM.NE.ZS': 'Labor force participation rate, total (15-24)',\n",
    "    'SL.EMP.TOTL.SP.FE.ZS': 'Employment to population ratio, 15+, female',\n",
    "    'SL.TLF.CACT.FE.NE.ZS': 'Labor force participation rate, female (15-24)',\n",
    "    'SL.UEM.NEET.FE.ME.ZS': 'Not in Education, Employment, or Training, female - Middle East',\n",
    "    'SL.UEM.NEET.FE.ZS': 'Not in Education, Employment, or Training, female',\n",
    "    'SL.UEM.NEET.ME.ZS': 'Not in Education, Employment, or Training, male - Middle East',\n",
    "    'EG.GDP.PUSE.KO.PP.KD': 'Energy use - Constant PPP',\n",
    "    'EG.GDP.PUSE.KO.PP': 'Energy use - Current PPP',\n",
    "    'SL.UEM.NEET.ZS': 'Not in Education, Employment, or Training, total',\n",
    "    'ER.H2O.FWST.ZS': 'Freshwater withdrawal'\n",
    "}\n",
    "# Finding the names of the principal components\n",
    "feature_names = features.columns.tolist()\n",
    "pc_interpretations = [\"\"] * 5 \n",
    "\n",
    "print(\"\\nPCA Component Interpretation:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# top 5 principal components\n",
    "max_components = min(5, len(pca_results))\n",
    "\n",
    "for n_components in range(2, max_components + 1):\n",
    "    pca_data = pca_results[n_components]\n",
    "    total_variance = np.sum(pca_data[\"explained_var\"]) * 100\n",
    "    \n",
    "    print(f\"\\nPCA with {n_components} components (Explains {total_variance:.1f}% total variance):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i in range(n_components):\n",
    "        component = pca_data[\"components\"][i]\n",
    "        variance_explained = pca_data[\"explained_var\"][i] * 100\n",
    "        print(f\"\\nPrincipal Component {i+1} ({variance_explained:.1f}% variance):\")\n",
    "        top_positive = np.argsort(component)[-7:][::-1]\n",
    "        top_negative = np.argsort(component)[:7]\n",
    "        \n",
    "        # positive features\n",
    "        print(\" + Strongly POSITIVE features (high values indicate):\")\n",
    "        pos_descriptions = [] # Store descriptions for interpretation\n",
    "        for idx in top_positive:\n",
    "            feat_name = feature_names[idx]\n",
    "            feat_desc = feature_descriptions.get(feat_name, feat_name)\n",
    "            pos_descriptions.append(feat_desc) # Add to list\n",
    "            print(f\"    • {feat_desc}: {component[idx]:.3f}\")\n",
    "        \n",
    "        # negative features\n",
    "        print(\" - Strongly NEGATIVE features (low values indicate):\")\n",
    "        neg_descriptions = [] # Store descriptions for interpretation\n",
    "        for idx in top_negative:\n",
    "            feat_name = feature_names[idx]\n",
    "            feat_desc = feature_descriptions.get(feat_name, feat_name)\n",
    "            neg_descriptions.append(feat_desc) # Add to list\n",
    "            print(f\"    • {feat_desc}: {component[idx]:.3f}\")\n",
    "        \n",
    "        all_descriptions = \" \".join(pos_descriptions + neg_descriptions).lower()\n",
    "        \n",
    "        #keyword groups\n",
    "        theme_keywords = {\n",
    "            'Demographic': ['population', 'birth', 'death', 'age', 'dependency', 'fertility', 'mortality', 'life expectancy'],\n",
    "            'Economic': ['gdp', 'gni', 'income', 'gross', 'product', 'consumption', 'expenditure', 'value added', 'account'],\n",
    "            'Labor/Social': ['labor', 'employment', 'unemployment', 'force', 'participation', 'neet', 'gender'],\n",
    "            'Trade/Financial': ['trade', 'export', 'import', 'investment', 'oda', 'financial', 'current account'],\n",
    "            'Environmental': ['co2', 'emission', 'energy', 'water', 'withdrawal', 'arable', 'land'],\n",
    "            'Inequality': ['gini', 'income share', 'inequality', 'poorest', 'richest']\n",
    "        }\n",
    "        \n",
    "        # themes based on keywords\n",
    "        component_themes = {}\n",
    "        for theme, keywords in theme_keywords.items():\n",
    "            count = sum(1 for keyword in keywords if keyword in all_descriptions)\n",
    "            if count > 0:\n",
    "                component_themes[theme] = count\n",
    "                \n",
    "        sorted_themes = sorted(component_themes.items(), key=lambda x: x[1], reverse=True)\n",
    "        main_themes = [theme for theme, count in sorted_themes[:2]] # Top 2 themes\n",
    "\n",
    "        if len(main_themes) == 2:\n",
    "            interpretation = f\"contrasts {main_themes[0]} with {main_themes[1]}\"\n",
    "        elif len(main_themes) == 1:\n",
    "            interpretation = f\"is primarily related to {main_themes[0]}\"\n",
    "        else:\n",
    "            interpretation = \"has mixed themes (interpret carefully)\"\n",
    "\n",
    "        pc_interpretations[i] = interpretation\n",
    "        print(f\" Interpretation: PC{i+1} {interpretation}\")\n",
    "\n",
    "# Radar chart of component contributions\n",
    "if max_components >= 3:\n",
    "    components_to_plot = min(5, max_components)\n",
    "    angles = np.linspace(0, 2 * np.pi, components_to_plot, endpoint=False).tolist()\n",
    "    angles += angles[:1] \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # Plot variance explained for each component\n",
    "    variances = pca_results[components_to_plot][\"explained_var\"][:components_to_plot] * 100\n",
    "    variances = np.append(variances, variances[0])\n",
    "    \n",
    "    ax.plot(angles, variances, 'o-', linewidth=2, markersize=8)\n",
    "    ax.fill(angles, variances, alpha=0.25)\n",
    "    labels = [f'PC{i+1}: {pc_interpretations[i]}' for i in range(components_to_plot)]\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_ylim(0, max(variances) * 1.1)\n",
    "    ax.set_title('Variance Explained by Principal Components', size=14, pad=20)\n",
    "    ax.grid(True)\n",
    "\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(variances[:-1]):\n",
    "        ax.text(angles[i], v + 1, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cc8c0f-601e-4b99-b8f5-6c592ad55693",
   "metadata": {},
   "source": [
    "## Part 6: Cluster Interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f38f2-a134-4ffa-b794-55d8b81d0125",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cluster Interpretation\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Cluster Interpretation Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Store results for markdown reporting\n",
    "cluster_results = []\n",
    "\n",
    "# 1. Detailed cluster analysis\n",
    "personas = [\n",
    "    \"Energy-Constrained Developing Economies\",\n",
    "    \"Environmentally-Managed Transition Economies\",\n",
    "    \"Specialized Niche Economic Systems\"\n",
    "]\n",
    "\n",
    "for cluster_id in range(len(clusters)):\n",
    "    print(f\"\\n Cluster {cluster_id} (Size: {len(clusters[cluster_id])} countries)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    centroid = centroids[cluster_id]\n",
    "    top_features = np.argsort(np.abs(centroid))[-5:][::-1]\n",
    "    \n",
    "    cluster_info = {\n",
    "        'id': cluster_id,\n",
    "        'size': len(clusters[cluster_id]),\n",
    "        'characteristics': [],\n",
    "        'pattern': \"\"\n",
    "    }\n",
    "    \n",
    "    print(\"Dominant characteristics:\")\n",
    "    for feat_idx in top_features:\n",
    "        feat_name = feature_names[feat_idx]\n",
    "        feat_desc = feature_descriptions.get(feat_name, feat_name)\n",
    "        value = centroid[feat_idx]\n",
    "        direction = \"HIGH\" if value > 0 else \"LOW\"\n",
    "        \n",
    "        char_desc = f\"{direction} {feat_desc}\"\n",
    "        cluster_info['characteristics'].append(char_desc)\n",
    "        print(f\"   • {char_desc}\")\n",
    "    \n",
    "    # Determine cluster pattern\n",
    "    pattern_parts = []\n",
    "    if any('EG.CFT.ACCS.ZS' in char for char in cluster_info['characteristics']):\n",
    "        pattern_parts.append(\"Energy-Access Focused\")\n",
    "    if any('SP.POP.DPND' in char for char in cluster_info['characteristics']):\n",
    "        pattern_parts.append(\"Demographic Dynamics\")\n",
    "    if any('NY.ADJ.DPEM.GN.ZS' in char for char in cluster_info['characteristics']):\n",
    "        pattern_parts.append(\"Environmental Impact\")\n",
    "    \n",
    "    cluster_info['pattern'] = \" + \".join(pattern_parts) if pattern_parts else \"Mixed Development\"\n",
    "    cluster_results.append(cluster_info)\n",
    "\n",
    "# 2. Print summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLUSTER SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for cluster in cluster_results:\n",
    "    print(f\"Cluster {cluster['id']}: {cluster['pattern']} ({cluster['size']} countries)\")\n",
    "\n",
    "# Store for markdown\n",
    "unique_patterns = len(set(cluster['pattern'] for cluster in cluster_results))\n",
    "smallest_cluster = min(cluster['size'] for cluster in cluster_results)\n",
    "largest_cluster = max(cluster['size'] for cluster in cluster_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLUSTER PERSONAS\")\n",
    "print(\"=\"*80)\n",
    "for cluster_id, persona in enumerate(personas):\n",
    "    print(f\"Cluster {cluster_id}: {persona}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5ac99c-1c27-4f22-a37b-9ec550479237",
   "metadata": {},
   "source": [
    "#### Cluster Interpretation Analysis\n",
    "\n",
    "###### Cluster Characteristics Summary\n",
    "\n",
    "Based on the k-means clustering analysis, three distinct country groups emerged:\n",
    "\n",
    "**Cluster 0**: Energy-Access Focused + Demographic Dynamics (115 countries)  \n",
    "- Characterized by challenges in energy access combined with specific demographic patterns\n",
    "- Represents a significant portion of developing economies\n",
    "\n",
    "**Cluster 1**: Environmental Impact + Mixed Development (337 countries)  \n",
    "- Shows distinctive environmental and development metrics\n",
    "- Largest cluster, indicating common development patterns\n",
    "\n",
    "**Cluster 2**: Specialized Group (17 countries)  \n",
    "- Much smaller group suggesting unique characteristics\n",
    "- May represent outlier nations or specialized economic structures\n",
    "\n",
    "###### Development Implications\n",
    "\n",
    "**Energy Access Patterns**: The separation of clusters based on energy metrics (EG.CFT.ACCS.ZS) highlights the fundamental role of energy infrastructure in development classification. Countries struggling with energy access form distinct groupings that cut across traditional geographic boundaries.\n",
    "\n",
    "**Demographic Influences**: The presence of demographic features (SP.POP.DPND) in cluster differentiation underscores how population structure influences development trajectories. Younger populations versus aging societies follow different development pathways.\n",
    "\n",
    "**Environmental Considerations**: Environmental metrics (NY.ADJ.DPEM.GN.ZS) contribute to cluster formation, suggesting that environmental management is increasingly a determinant in development patterns.\n",
    "\n",
    "###### Comparison with HDI Framework\n",
    "\n",
    "While the Human Development Index provides a linear ranking, our clustering reveals multidimensional groupings:\n",
    "\n",
    "- **Complementary Insights**: Clusters identify countries with similar development constraints and opportunities, regardless of their absolute HDI ranking\n",
    "- **Policy Relevance**: Cluster-based analysis may better inform targeted development interventions than ordinal rankings\n",
    "- **Pattern Recognition**: The emergence of distinct groups suggests common development challenges that transcend national boundaries\n",
    "\n",
    "###### Limitations and Future Analysis\n",
    "\n",
    "The current analysis reveals several avenues for further investigation:\n",
    "- Temporal analysis could track cluster transitions as countries develop\n",
    "- Inclusion of additional governance and institutional metrics might refine clusters\n",
    "- Comparative analysis with World Bank income classifications could provide additional validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aaea10-1371-4e2a-8547-1860075d0dde",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing analysis with a comparison plot: \n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "all_feature_importance = []\n",
    "for cluster_id, centroid in enumerate(centroids):\n",
    "    for feat_idx, value in enumerate(centroid):\n",
    "        importance = abs(value)\n",
    "        all_feature_importance.append((importance, feat_idx, cluster_id))\n",
    "\n",
    "all_feature_importance.sort(reverse=True)\n",
    "top_feature_indices = list(dict.fromkeys([feat_idx for _, feat_idx, _ in all_feature_importance]))[:6]  # Top 6 features\n",
    "\n",
    "for cluster_id, centroid in enumerate(centroids):\n",
    "    values = [centroid[idx] for idx in top_feature_indices]\n",
    "    feature_names_short = [feature_names[idx][:15] + '...' if len(feature_names[idx]) > 15 else feature_names[idx] \n",
    "                          for idx in top_feature_indices]\n",
    "    \n",
    "    plt.plot(range(len(values)), values, 'o-', label=f'Cluster {cluster_id}', \n",
    "             linewidth=2, markersize=8, alpha=0.8)\n",
    "\n",
    "plt.xlabel('Most Important Features')\n",
    "plt.ylabel('Centroid Value (Standardized)')\n",
    "plt.title('Cluster Profiles Based on Most Distinctive Features')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xticks(range(len(top_feature_indices)), feature_names_short, rotation=45, ha='right')\n",
    "plt.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ce4b8-422b-4ee3-8035-da89ebd2bf3b",
   "metadata": {},
   "source": [
    "## Part 7.1: Outlier Analysis & Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70ad749-0325-467a-8d7b-0e93b54cecb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTLIER ANALYSIS\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OUTLIER ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Method 1: Z-score based outlier detection\n",
    "from scipy import stats\n",
    "z_scores = np.abs(stats.zscore(X))\n",
    "outlier_mask = (z_scores > 3).any(axis=1)  # Points with any feature > 3 std dev\n",
    "\n",
    "print(f\"Number of potential outliers (Z-score > 3): {outlier_mask.sum()}\")\n",
    "print(f\"Percentage of data: {outlier_mask.sum()/len(X)*100:.1f}%\")\n",
    "\n",
    "# Method 2: Isolation Forest (more robust)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "outlier_labels = iso_forest.fit_predict(X)\n",
    "outlier_mask_iso = outlier_labels == -1\n",
    "\n",
    "print(f\"Number of outliers (Isolation Forest): {outlier_mask_iso.sum()}\")\n",
    "print(f\"Percentage of data: {outlier_mask_iso.sum()/len(X)*100:.1f}%\")\n",
    "\n",
    "# Remove outliers and re-cluster\n",
    "X_clean = X[~outlier_mask_iso]\n",
    "\n",
    "print(f\"\\nOriginal data shape: {X.shape}\")\n",
    "print(f\"Clean data shape (outliers removed): {X_clean.shape}\")\n",
    "\n",
    "# Re-run k-means on clean data\n",
    "print(\"\\nRe-running k-means on data without outliers...\")\n",
    "k_clean = k_optimal  # Use same k for comparison\n",
    "\n",
    "# Use your existing kmeans function\n",
    "clusters_clean, centroids_clean, count_clean, inertia_clean, shift_clean = kmeans(\n",
    "    X_clean, k_clean, k_means_plus_plus(X_clean, k_clean), \n",
    "    max_it=300, c_change_min=0.0001, inertia_change_min=0.1, stop='centroid'\n",
    ")\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: WITH vs WITHOUT OUTLIERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"{'Metric':<25} {'With Outliers':<15} {'Without Outliers':<15} {'Change':<10}\")\n",
    "print(\"-\" * 65)\n",
    "print(f\"{'Total countries':<25} {len(X):<15} {len(X_clean):<15} {len(X_clean)-len(X):<10}\")\n",
    "print(f\"{'Inertia':<25} {inertia_history[-1]:<15.1f} {inertia_clean[-1]:<15.1f} {inertia_clean[-1]-inertia_history[-1]:<10.1f}\")\n",
    "\n",
    "# Compare cluster sizes\n",
    "print(f\"\\nCluster size distribution:\")\n",
    "print(f\"{'':<25} {'Original':<15} {'Clean':<15}\")\n",
    "for i in range(k_clean):\n",
    "    orig_size = len(clusters[i])\n",
    "    clean_size = len(clusters_clean[i]) if i < len(clusters_clean) else 0\n",
    "    print(f\"Cluster {i}:{''*(25-len(f'Cluster {i}'))} {orig_size:<15} {clean_size:<15} {clean_size-orig_size:<+10}\")\n",
    "\n",
    "# Visual comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Original clusters\n",
    "sizes_orig = [len(c) for c in clusters]\n",
    "ax1.bar(range(len(sizes_orig)), sizes_orig, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.set_title('Cluster Sizes (With Outliers)')\n",
    "ax1.set_xlabel('Cluster')\n",
    "ax1.set_ylabel('Number of Countries')\n",
    "\n",
    "# Clean clusters  \n",
    "sizes_clean = [len(c) for c in clusters_clean]\n",
    "ax2.bar(range(len(sizes_clean)), sizes_clean, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "ax2.set_title('Cluster Sizes (Without Outliers)')\n",
    "ax2.set_xlabel('Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey insights from outlier removal:\")\n",
    "print(\"• Outliers typically: Very small rich countries, conflict zones, data anomalies\")\n",
    "print(\"• Removal often: Reduces inertia, stabilizes clusters, improves interpretation\")\n",
    "print(\"• Effect: Usually makes clusters more homogeneous and patterns clearer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b44492-1f4b-4edc-ac6b-9b51fcc589e4",
   "metadata": {},
   "source": [
    "## Part 7.2: Alternative clustering algorithm (DBSCAN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e08413-c087-4381-b93d-5efe93621f8c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# DBSCAN\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DBSCAN ALGORITHM COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale data for DBSCAN (distance-based)\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Find good DBSCAN parameters using k-distance graph\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn = NearestNeighbors(n_neighbors=5)\n",
    "nn.fit(X_scaled)\n",
    "distances, indices = nn.kneighbors(X_scaled)\n",
    "k_distances = np.sort(distances[:, -1])[::-1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_distances)\n",
    "plt.xlabel('Points sorted by distance')\n",
    "plt.ylabel('5th nearest neighbor distance')\n",
    "plt.title('k-Distance Graph for DBSCAN parameter selection')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Try DBSCAN with different parameters (FIXED - no random_state)\n",
    "dbscan_params = [\n",
    "    {'eps': 0.5, 'min_samples': 5},\n",
    "    {'eps': 1.0, 'min_samples': 5}, \n",
    "    {'eps': 1.5, 'min_samples': 5},\n",
    "    {'eps': 2.0, 'min_samples': 5}\n",
    "]\n",
    "\n",
    "dbscan_results = []\n",
    "for params in dbscan_params:\n",
    "    dbscan = DBSCAN(**params)  # REMOVED random_state\n",
    "    labels = dbscan.fit_predict(X_scaled)\n",
    "    \n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "    \n",
    "    # Silhouette score (excluding noise)\n",
    "    if n_clusters > 1:\n",
    "        valid_mask = labels != -1\n",
    "        if valid_mask.sum() > 0:  # Ensure we have points to score\n",
    "            silhouette = silhouette_score(X_scaled[valid_mask], labels[valid_mask])\n",
    "        else:\n",
    "            silhouette = -1\n",
    "    else:\n",
    "        silhouette = -1\n",
    "    \n",
    "    dbscan_results.append({\n",
    "        'params': params,\n",
    "        'n_clusters': n_clusters,\n",
    "        'n_noise': n_noise,\n",
    "        'silhouette': silhouette,\n",
    "        'labels': labels\n",
    "    })\n",
    "    \n",
    "    print(f\"DBSCAN {params}: {n_clusters} clusters, {n_noise} noise points, silhouette: {silhouette:.3f}\")\n",
    "\n",
    "# Compare with k-means\n",
    "best_dbscan = max(dbscan_results, key=lambda x: x['silhouette'] if x['silhouette'] != -1 else -10)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"K-MEANS vs DBSCAN COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"{'Metric':<20} {'K-Means':<15} {'DBSCAN (best)':<15}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Number of clusters':<20} {k_optimal:<15} {best_dbscan['n_clusters']:<15}\")\n",
    "print(f\"{'Noise points':<20} {'0':<15} {best_dbscan['n_noise']:<15}\")\n",
    "print(f\"{'Silhouette score':<20} {silhouette_scores[k_optimal-2]:<15.3f} {best_dbscan['silhouette']:<15.3f}\")\n",
    "\n",
    "# Visual comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# K-means results (using your existing cluster labels)\n",
    "kmeans_labels = np.zeros(len(X))\n",
    "for i, cluster in enumerate(clusters):\n",
    "    for point in cluster:\n",
    "        idx = np.where((X == point).all(axis=1))[0]\n",
    "        if len(idx) > 0:\n",
    "            kmeans_labels[idx[0]] = i\n",
    "\n",
    "# Plot k-means\n",
    "scatter1 = ax1.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.6)\n",
    "ax1.set_title(f'K-Means (k={k_optimal}, silhouette={silhouette_scores[k_optimal-2]:.3f})')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "plt.colorbar(scatter1, ax=ax1)\n",
    "\n",
    "# Plot DBSCAN\n",
    "scatter2 = ax2.scatter(X[:, 0], X[:, 1], c=best_dbscan['labels'], cmap='viridis', alpha=0.6)\n",
    "ax2.set_title(f'DBSCAN (eps={best_dbscan[\"params\"][\"eps\"]}, clusters={best_dbscan[\"n_clusters\"]})')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "plt.colorbar(scatter2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19eab9f-db1e-41de-b9da-1edabe8875ff",
   "metadata": {},
   "source": [
    "##### K-MEANS:\n",
    "✓ Strengths: Fast, scalable, works well with spherical clusters\n",
    "✗ Weaknesses: Requires specifying k, sensitive to outliers, assumes spherical clusters\n",
    "\n",
    "##### DBSCAN:\n",
    "✓ Strengths: No need to specify clusters, handles noise, finds arbitrary shapes  \n",
    "✗ Weaknesses: Sensitive to parameters, struggles with varying density clusters\n",
    "\n",
    "###### CONCLUSION:\n",
    "• K-means is better for: Well-separated spherical clusters, known number of clusters\n",
    "• DBSCAN is better for: Noise detection, arbitrary cluster shapes, unknown cluster count\n",
    "• Your data appears to: Work well with both methods, suggesting a clear cluster structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d494769-9c96-48ea-b715-5f8556e96b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
